回归
1.回归于分类的区别在于分类和回归的区别在于输出变量的类型。分类的输出是离散的，回归的输出是连续的。定量输出称为回归，或者说是连续变量预测；
定性输出称为分类，或者说是离散变量预测。

2.线性回归
线性回归就是将输入项分别乘以一些常量(回归系数)，再将结果加起来得到输出。求回归系数的过程就叫做回归。
线性回归的数学推导：
通过比较相关系数可以比较线性回归模型的好坏，相关系数的计算公式为：

3.局部加权线性回归
线性回归的一个问题是有可能出现欠拟合现象

4.岭回归(ridge regression)
岭回归就是在矩阵上加一个从而使得矩阵非奇异，从而就能对矩阵进行求逆。

5.前向逐步回归
6.lasso法


7，线性回归和局部加权线性回归的区别理解：
线性回归是一种 parametric learning algorithm。Parametric learning algorithm 有固定的（指的是：值的大小是固定）、有限的参数，通过训练样
本，找到合适的参数后，对于之后未知的输入，我们可以直接利用这组参数得出其相应的预测输出，参数一旦确定，就不会改变了，我们不需要在保留训练集中
的训练样本。
局部加权线性回归是一种 non-parametric learning algorithm。non-parametric learning algorithm 需要的计算量与输入的训练集大小成正比，对于
每次新的输入，需重新计算相应参数后，才能求取相应的预测输出，所以需要一直保留训练样本。
线性回归容易出现欠拟合的情况，因为线性回归求解的是具有最小均方误差的无偏估计(关于无偏估计就是用样本统计量来估计总体参数时的一种无偏推断。估计
量的数学期望等于被估计参数的真实值，则称此此估计量为被估计参数的无偏估计，即具有无偏性)，是用所有的样本数据得出的结论，所以如果模型欠拟合将不
能取得最好的预测结果，所以有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。

8.KNN邻近算法
KNN邻近算法采用测量不同特征值之间的距离方法进行分类，它的工作原理是：存在一个样本数据集合，也称为训练样本集，并且样本集中每个数据都存在标签，
即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样
本集中特征最相似数据（最临近）的分类标签。一般来说我们只选择样本数据集中前K个最相似的数据，这就是KNN邻近算法中K的出处。最后，选择K个最相似数
据中出现次数最多的分类，作为新数据的分类。
